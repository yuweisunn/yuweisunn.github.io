<html>
<meta name="viewport" content="width=device-width, initial-scale=1">
<head>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" />

<style>
.content {
  max-width: 50%;
  margin: auto;
  text-align: justify;
}
h2{color:#990000}
a:link {
  text-decoration: none;
}

footer{position: fixed;
  bottom: 50;}
</style>
<title>Yuwei Sun | blog</title>

</head>
  
<body class="content">   
  <br>
<h1>Decentralized Artificial Intelligence Blog</h1>
Created by <a href="https://yuweisunn.github.io/"><b>Yuwei Sun</b></a>
<br><br><br>
<div class="row">
  <div class="leftcolumn">

    <div class="card">
      <a href="blog-4-12.html">
      <h2 style="font-size:24px;"><b>Self-Supervised Learning and Multimodal Learning</b></h2></a>
      Apr 12, 2022
      <p>Information in the real world usually comes as different modalities. When
      searching for visual or audio content on the web, we can train a model
      leveraging any available collection of web data and index that type of
      media based on learned multimodal embeddings.</p>
    </div>
  </div>
</div>
</body>
<footer>
    <p>Created by Yuwei Sun. Â© 2022</p>
</footer>
<br>
</html>
